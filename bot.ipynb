{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import stable_baselines3\n",
    "import gym_trading_env\n",
    "from gym_trading_env.renderer import Renderer   \n",
    "from pathlib import Path\n",
    "from BitcoinDownloader import download_exchange_data, get_dataframes\n",
    "from BitcoinIndicators import Indicators\n",
    "from BitcoinRewards import reward_function\n",
    "from BitcoinDynamicFeatures import dynamic_feature_real_position, dynamic_feature_last_position_taken\n",
    "\n",
    "import torch\n",
    "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Data if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data already downloaded\n"
     ]
    }
   ],
   "source": [
    "download_again = False\n",
    "data_path = './data/binance-BTCUSDT-1h.pkl'\n",
    "if not Path(data_path).is_file() or download_again:\n",
    "    download_exchange_data()\n",
    "else:\n",
    "    print(\"data already downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df, testing_df = get_dataframes()\n",
    "print(training_df.head(2))\n",
    "print(training_df.tail(2))\n",
    "print(testing_df.head(2))\n",
    "print(testing_df.tail(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_path = './data/indicators.csv'\n",
    "training_df.to_csv(ind_path)\n",
    "indicators = Indicators(ind_path)\n",
    "indicators.to_csv(ind_path)\n",
    "training_df = pd.read_csv(ind_path)\n",
    "training_df[\"date_open\"] = pd.to_datetime(training_df[\"date_open\"])\n",
    "training_df.set_index(\"date_open\", inplace=True)\n",
    "\n",
    "training_df.dropna(inplace=True)\n",
    "print(training_df.head(3))\n",
    "print(training_df.tail(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_env = gym.make(\"TradingEnv\",\n",
    "        name= \"BTCUSD\",\n",
    "        df = training_df, # Your dataset with your custom features\n",
    "        positions = [0, 1], # -1 (=SHORT), 0(=SELL ALL), +1 (=BUY ALL)\n",
    "        #trading_fees = 0.01/100, # 0.01% per stock buy / sell (Binance fees)\n",
    "        #borrow_interest_rate= 0.0003/100, # 0.0003% per timestep (one timestep = 1h here)\n",
    "        #dynamic_feature_functions = [dynamic_features]\n",
    "        dynamic_features = [dynamic_feature_last_position_taken, dynamic_feature_real_position],\n",
    "        portfolio_initial_value = 1000,\n",
    "        reward_function = reward_function\n",
    "        #max_episode_duration = 1000,\n",
    "    )\n",
    "\n",
    "observation, info = training_env.reset()\n",
    "print(observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RecurrentPPO' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m RecurrentPPO(\u001b[39m'\u001b[39m\u001b[39mMlpLstmPolicy\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m# feed-forward neural network with multiple hidden layers\u001b[39;00m\n\u001b[0;32m      2\u001b[0m             training_env, \u001b[39m# environment in which the agent interacts and learns\u001b[39;00m\n\u001b[0;32m      3\u001b[0m             verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m# enables the training progress to be printed during the learning process\u001b[39;00m\n\u001b[0;32m      4\u001b[0m             gamma\u001b[39m=\u001b[39m\u001b[39m0.95\u001b[39m, \u001b[39m# determines the importance of future rewards compared to immediate rewards\u001b[39;00m\n\u001b[0;32m      5\u001b[0m             n_steps\u001b[39m=\u001b[39m\u001b[39m15\u001b[39m, \u001b[39m# steps to collect samples from the environment before performing an update\u001b[39;00m\n\u001b[0;32m      6\u001b[0m             ent_coef\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m, \u001b[39m# encourages exploration by adding entropy to the policy loss\u001b[39;00m\n\u001b[0;32m      7\u001b[0m             learning_rate\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m, \u001b[39m# controls the step size at which model's parameters are updated based on the gradient of the loss function\u001b[39;00m\n\u001b[0;32m      8\u001b[0m             clip_range\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, \u001b[39m# limits the update to a certain range to prevent large policy updates\u001b[39;00m\n\u001b[0;32m      9\u001b[0m             device\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(training_df)\n\u001b[0;32m     11\u001b[0m model\u001b[39m.\u001b[39mlearn(\u001b[39m30000\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RecurrentPPO' is not defined"
     ]
    }
   ],
   "source": [
    "model = RecurrentPPO('MlpLstmPolicy', # feed-forward neural network with multiple hidden layers\n",
    "            training_env, # environment in which the agent interacts and learns\n",
    "            verbose=1, # enables the training progress to be printed during the learning process\n",
    "            gamma=0.95, # determines the importance of future rewards compared to immediate rewards\n",
    "            n_steps=15, # steps to collect samples from the environment before performing an update\n",
    "            ent_coef=0.01, # encourages exploration by adding entropy to the policy loss\n",
    "            learning_rate=0.001, # controls the step size at which model's parameters are updated based on the gradient of the loss function\n",
    "            clip_range=0.1, # limits the update to a certain range to prevent large policy updates\n",
    "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n = len(training_df)\n",
    "model.learn(30000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate indicators for testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_path = './data/indicators.csv'\n",
    "testing_df.to_csv(ind_path)\n",
    "indicators = Indicators(ind_path)\n",
    "indicators.to_csv(ind_path)\n",
    "testing_df = pd.read_csv(ind_path)\n",
    "testing_df[\"date_open\"] = pd.to_datetime(testing_df[\"date_open\"])\n",
    "testing_df.set_index(\"date_open\", inplace=True)\n",
    "\n",
    "testing_df.dropna(inplace=True)\n",
    "print(testing_df.head(3))\n",
    "print(testing_df.tail(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_env = gym.make(\"TradingEnv\",\n",
    "        name= \"BTCUSD\",\n",
    "        df = testing_df, # Your dataset with your custom features\n",
    "        positions = [0, 1], # -1 (=SHORT), 0(=SELL ALL), +1 (=BUY ALL)\n",
    "        portfolio_initial_value = 1000,\n",
    "    )\n",
    "\n",
    "observation, info = testing_env.reset()\n",
    "print(observation.shape)\n",
    "print(observation)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test trained model on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(len(testing_df)):\n",
    "    position_index, _states = model.predict(observation)\n",
    "    observation, reward, done, truncated, info = testing_env.step(position_index)\n",
    "    testing_env.save_for_render(dir = \"render_logs\")\n",
    "    if done or truncated:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Render results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renderer = Renderer(render_logs_dir=\"render_logs\")\n",
    "renderer.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
