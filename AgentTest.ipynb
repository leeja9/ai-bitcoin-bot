{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Till Zemann\n",
    "# License: MIT License\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from matplotlib.patches import Patch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gymnasium as gym\n",
    "import gym_trading_env\n",
    "from gym_trading_env.utils.history import History\n",
    "from gym_trading_env.downloader import download\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create features\n",
    "\n",
    "- These are the state vector inputs\n",
    "- this would be an import from `ta_indicators`\n",
    "- only data columns starting with \"`feature_`\" will appear in the observation space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 7 (2473844012.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[50], line 8\u001b[1;36m\u001b[0m\n\u001b[1;33m    \"\"\"Deprecated.  I changed ta_indicators to put 'feature_' in front of all final indicators.\"\"\"\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after function definition on line 7\n"
     ]
    }
   ],
   "source": [
    "from ta_indicators import * # Import the indicators\n",
    "\n",
    "# df is a DataFrame with columns : \"open\", \"high\", \"low\", \"close\", \"Volume USD\"\n",
    "\n",
    "# this can be defined in TAindicators and imported directly to the agent\n",
    "# after we get through testing things out in notebooks\n",
    "def define_state_vector(df: pd.DataFrame):\n",
    "\"\"\"Deprecated.  I changed ta_indicators to put 'feature_' in front of all final indicators.\"\"\"\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # Create the feature : ( close[t] - close[t-1] )/ close[t-1]\n",
    "    df[\"feature_close\"] = df[\"close\"].pct_change()\n",
    "\n",
    "    # Create the feature : open[t] / close[t]\n",
    "    df[\"feature_open\"] = df[\"open\"]/df[\"close\"]\n",
    "\n",
    "    # Create the feature : high[t] / close[t]\n",
    "    df[\"feature_high\"] = df[\"high\"]/df[\"close\"]\n",
    "\n",
    "    # Create the feature : low[t] / close[t]\n",
    "    df[\"feature_low\"] = df[\"low\"]/df[\"close\"]\n",
    "\n",
    "    # Create the feature : volume[t] / max(*volume[t-7*24:t+1])\n",
    "    df[\"feature_volume\"] = df[\"Volume USD\"] / df[\"Volume USD\"].rolling(7*24).max()\n",
    "\n",
    "    df.dropna(inplace= True) # Clean again !\n",
    "    # Eatch step, the environment will return 5 inputs  : \"feature_close\", \"feature_open\", \"feature_high\", \"feature_low\", \"feature_volume\"\n",
    "    \"\"\"\n",
    "\n",
    "df = pd.read_csv(\"./data/indicators.csv\")\n",
    "df.dropna(inplace= True)\n",
    "\n",
    "#df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define custom rewards and dynamic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_reward_columns(df: pd.DataFrame):\n",
    "    \"\"\"add reward columns to dataframe for incremental updates\"\"\"\n",
    "    for col in ['lr', 'alr', 'var_sum']:\n",
    "        df[col] = 0\n",
    "\n",
    "def update_reward_columns(history: History) -> None:\n",
    "    \"\"\"Set this episode lr, alr, var_sum, sr, powc\"\"\"\n",
    "\n",
    "    # Using weighted incremental algorithmic approach for average\n",
    "    # https://math.stackexchange.com/questions/106700/incremental-averaging\n",
    "    # general formula is: mean = ((n - 1) * last_mean + this_value) / n))\n",
    "\n",
    "    # logarithmic return\n",
    "    this_lr = 0\n",
    "    # if position is 1 (100% BTC)\n",
    "    if history['position', -1] == 1:\n",
    "        this_lr = np.log(history['data_close', -1]) - np.log(history['data_close', -2])\n",
    "    history.__setitem__(('data_lr', -1), this_lr) # update history with new lr\n",
    "\n",
    "\n",
    "    # running average of logarithmic return\n",
    "    n = len(history)\n",
    "    last_alr = history['data_alr', -2]\n",
    "    this_alr = ((n - 1) * last_alr + this_lr) / n\n",
    "    history.__setitem__(('data_alr', -1), this_alr) # update history with new alr\n",
    "\n",
    "    # running variance sum of logarithmic return\n",
    "    # for each nth row, dividing this sum by n gives population variance\n",
    "    last_alr = history['data_alr', -2]\n",
    "    last_var_sum = history['data_var_sum', -2]\n",
    "    this_var_sum = last_var_sum + abs((this_lr - last_alr) * (this_lr - this_alr))\n",
    "    history.__setitem__(('data_var_sum', -1), this_var_sum)\n",
    "\n",
    "def get_random_weights(arr_len):\n",
    "    \"\"\"get numpy array of random weights\"\"\"\n",
    "    max_val = 100\n",
    "    weight_vector = np.zeros(arr_len)\n",
    "    for i in range(arr_len - 1):\n",
    "        n = np.random.randint(0, max_val)\n",
    "        max_val = max_val - n\n",
    "        weight_vector[i] = n\n",
    "    weight_vector /= 100\n",
    "    weight_vector[-1] = 1 - sum(weight_vector[:-1])\n",
    "    np.random.shuffle(weight_vector)\n",
    "    return weight_vector\n",
    "\n",
    "\n",
    "def reward_function(history: History) -> float:\n",
    "    \"\"\"reward function for gym-trading-env\"\"\"\n",
    "    update_reward_columns(history)\n",
    "    average_log_return = history['data_alr', -1]\n",
    "    var_sum = history['data_var_sum', -1]\n",
    "    variance = var_sum / len(history)\n",
    "    std_dev = np.sqrt(variance)\n",
    "    sharpe_ratio = average_log_return / std_dev\n",
    "    this_lr = history['data_lr', -1]\n",
    "    powc = 0\n",
    "    # if this eposide position is 0 (100% USD) and last position was 1 (100% BTC)\n",
    "    # this compute time can also be traded for memory by adding a tracking column if needed\n",
    "    if (history['position', -1] == 0 and history['position', -2] == 1):\n",
    "        idx = history[-2]['idx']\n",
    "        while idx >= 0:\n",
    "            if (history['position', idx] == 0):\n",
    "                last_lr = history['data_lr', idx + 1]\n",
    "                powc = this_lr - last_lr\n",
    "    reward_vector = np.array([average_log_return, sharpe_ratio, powc])\n",
    "    weight_vector = get_random_weights(len(reward_vector))\n",
    "    reward = reward_vector @ weight_vector # dot product of random weights and reward values\n",
    "    return reward\n",
    "\n",
    "def dynamic_features(history: Histroy) -> float:\n",
    "    \"\"\"Calculates dynamic features.\"\"\"\n",
    "    #dyn_features = [last_position, real_position]\n",
    "    #return dyn_features\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.61 0.34 0.   0.02 0.01 0.02]\n",
      "[0.61 0.   0.02 0.02 0.34 0.01]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "max_val = 100\n",
    "w = np.zeros(6)\n",
    "for i in range(5):\n",
    "    n = np.random.randint(0, max_val)\n",
    "    max_val = max_val - n\n",
    "    w[i] = n\n",
    "w /= 100\n",
    "w[-1] = 1 - sum(w[:-1])\n",
    "print(w)\n",
    "np.random.shuffle(w)\n",
    "print(w)\n",
    "print(sum(w))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>close</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>vol</th>\n",
       "      <th>ROC_2</th>\n",
       "      <th>ROC_4</th>\n",
       "      <th>ROC_6</th>\n",
       "      <th>ROC_8</th>\n",
       "      <th>...</th>\n",
       "      <th>RSI_16</th>\n",
       "      <th>RSI_32</th>\n",
       "      <th>RSI_64</th>\n",
       "      <th>ATR_2</th>\n",
       "      <th>ATR_4</th>\n",
       "      <th>ATR_6</th>\n",
       "      <th>ATR_8</th>\n",
       "      <th>ATR_16</th>\n",
       "      <th>ATR_32</th>\n",
       "      <th>ATR_64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-08-29</td>\n",
       "      <td>4587.5</td>\n",
       "      <td>4386.7</td>\n",
       "      <td>4625.9</td>\n",
       "      <td>4313.5</td>\n",
       "      <td>0.60K</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-08-30</td>\n",
       "      <td>4555.1</td>\n",
       "      <td>4587.5</td>\n",
       "      <td>4647.5</td>\n",
       "      <td>4416.0</td>\n",
       "      <td>0.81K</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>271.95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-08-31</td>\n",
       "      <td>4724.9</td>\n",
       "      <td>4555.1</td>\n",
       "      <td>4745.4</td>\n",
       "      <td>4555.1</td>\n",
       "      <td>0.56K</td>\n",
       "      <td>2.831850</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>210.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>4834.9</td>\n",
       "      <td>4724.9</td>\n",
       "      <td>4885.5</td>\n",
       "      <td>4654.9</td>\n",
       "      <td>0.56K</td>\n",
       "      <td>6.154441</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>210.45</td>\n",
       "      <td>241.200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-09-02</td>\n",
       "      <td>4472.1</td>\n",
       "      <td>4834.9</td>\n",
       "      <td>4939.2</td>\n",
       "      <td>4286.9</td>\n",
       "      <td>0.93K</td>\n",
       "      <td>-4.783540</td>\n",
       "      <td>-0.406863</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>441.45</td>\n",
       "      <td>326.175</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date   close    open    high     low    vol     ROC_2     ROC_4  \\\n",
       "0  2017-08-29  4587.5  4386.7  4625.9  4313.5  0.60K       NaN       NaN   \n",
       "1  2017-08-30  4555.1  4587.5  4647.5  4416.0  0.81K       NaN       NaN   \n",
       "2  2017-08-31  4724.9  4555.1  4745.4  4555.1  0.56K  2.831850       NaN   \n",
       "3  2017-09-01  4834.9  4724.9  4885.5  4654.9  0.56K  6.154441       NaN   \n",
       "4  2017-09-02  4472.1  4834.9  4939.2  4286.9  0.93K -4.783540 -0.406863   \n",
       "\n",
       "   ROC_6  ROC_8  ...  RSI_16  RSI_32  RSI_64   ATR_2    ATR_4  ATR_6  ATR_8  \\\n",
       "0    NaN    NaN  ...     NaN     NaN     NaN     NaN      NaN    NaN    NaN   \n",
       "1    NaN    NaN  ...     NaN     NaN     NaN  271.95      NaN    NaN    NaN   \n",
       "2    NaN    NaN  ...     NaN     NaN     NaN  210.90      NaN    NaN    NaN   \n",
       "3    NaN    NaN  ...     NaN     NaN     NaN  210.45  241.200    NaN    NaN   \n",
       "4    NaN    NaN  ...     NaN     NaN     NaN  441.45  326.175    NaN    NaN   \n",
       "\n",
       "   ATR_16  ATR_32  ATR_64  \n",
       "0     NaN     NaN     NaN  \n",
       "1     NaN     NaN     NaN  \n",
       "2     NaN     NaN     NaN  \n",
       "3     NaN     NaN     NaN  \n",
       "4     NaN     NaN     NaN  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = pd.read_csv(\"./data/indicators.csv\")\n",
    "\n",
    "\"\"\"trainingDF = df.truncate(\n",
    "    after = pd.Timestamp('2023-01-01'),\n",
    "    copy = True\n",
    ")\n",
    "testingDF = df.truncate(\n",
    "    before = pd.Timestamp('2023-01-01'),\n",
    "    copy = True\n",
    ")\"\"\"\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndf = pd.read_pickle('./data/binance-BTCUSDT-1h.pkl')\\ntrainingDF = df.truncate(\\n    after = pd.Timestamp('2023-01-01'),\\n    copy = True\\n)\\n\\ntrainingDF.dropna(inplace=True)\\ntrainingDF.sort_index(inplace=True)\\ntrainingDF.to_csv('./data/binance_training_data.csv')\\n\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "df = pd.read_pickle('./data/binance-BTCUSDT-1h.pkl')\n",
    "trainingDF = df.truncate(\n",
    "    after = pd.Timestamp('2023-01-01'),\n",
    "    copy = True\n",
    ")\n",
    "\n",
    "trainingDF.dropna(inplace=True)\n",
    "trainingDF.sort_index(inplace=True)\n",
    "trainingDF.to_csv('./data/binance_training_data.csv')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"df = pd.read_pickle('./data/binance-BTCUSDT-1h.pkl')\\ntestingDF = df.truncate(\\n    before = pd.Timestamp('2023-01-01'),\\n    copy = True\\n)\\n\\ntestingDF.dropna(inplace=True)\\ntestingDF.sort_index(inplace=True)\\ntestingDF.to_csv('./data/binance_testing_data.csv')\\n\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"df = pd.read_pickle('./data/binance-BTCUSDT-1h.pkl')\n",
    "testingDF = df.truncate(\n",
    "    before = pd.Timestamp('2023-01-01'),\n",
    "    copy = True\n",
    ")\n",
    "\n",
    "testingDF.dropna(inplace=True)\n",
    "testingDF.sort_index(inplace=True)\n",
    "testingDF.to_csv('./data/binance_testing_data.csv')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'download(\\n    exchange_names = [\"bitfinex2\", \"huobi\"],\\n    symbols= [\"BTC/USDT\"],\\n    timeframe= \"1h\",\\n    dir = \"data\",\\n    since= datetime.datetime(year= 2017, month= 1, day=1)\\n)'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"download(\n",
    "    exchange_names = [\"bitfinex2\", \"huobi\"],\n",
    "    symbols= [\"BTC/USDT\"],\n",
    "    timeframe= \"1h\",\n",
    "    dir = \"data\",\n",
    "    since= datetime.datetime(year= 2017, month= 1, day=1)\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"huobiDF = pd.read_pickle('./data/huobi-BTCUSDT-1h.pkl')\\nhuobi_training = huobiDF.truncate(after='2023-01-01')\\nhuobi_training.to_csv('./data/huobi-BTCUSDT-1h-training.csv')\\nhuobi_test = huobiDF.truncate(before='2023-01-01')\\nhuobi_test.to_csv('./data/huobi-BTCUSDT-1h-test.csv')\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"huobiDF = pd.read_pickle('./data/huobi-BTCUSDT-1h.pkl')\n",
    "huobi_training = huobiDF.truncate(after='2023-01-01')\n",
    "huobi_training.to_csv('./data/huobi-BTCUSDT-1h-training.csv')\n",
    "huobi_test = huobiDF.truncate(before='2023-01-01')\n",
    "huobi_test.to_csv('./data/huobi-BTCUSDT-1h-test.csv')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"bitfinex2DF = pd.read_pickle('./data/bitfinex2-BTCUSDT-1h.pkl')\\nbitfinex2_training = bitfinex2DF.truncate(after='2023-01-01')\\nbitfinex2_test = bitfinex2DF.truncate(before='2023-01-01')\\nbitfinex2_training.to_csv('./data/bitfinex2-BTCUSDT-1h-training.csv')\\nbitfinex2_test.to_csv('./data/bitfinex2-BTCUSDT-1h-test.csv')\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"bitfinex2DF = pd.read_pickle('./data/bitfinex2-BTCUSDT-1h.pkl')\n",
    "bitfinex2_training = bitfinex2DF.truncate(after='2023-01-01')\n",
    "bitfinex2_test = bitfinex2DF.truncate(before='2023-01-01')\n",
    "bitfinex2_training.to_csv('./data/bitfinex2-BTCUSDT-1h-training.csv')\n",
    "bitfinex2_test.to_csv('./data/bitfinex2-BTCUSDT-1h-test.csv')\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"TradingEnv\",\n",
    "        name= \"BTCUSD\",\n",
    "        df = df, # Your dataset with your custom features\n",
    "        positions = [0, 1], # -1 (=SHORT), 0(=SELL ALL), +1 (=BUY ALL)\n",
    "        #trading_fees = 0.01/100, # 0.01% per stock buy / sell (Binance fees)\n",
    "        #borrow_interest_rate= 0.0003/100, # 0.0003% per timestep (one timestep = 1h here)\n",
    "        #dynamic_feature_functions = [dynamic_features]\n",
    "        reward_function = reward_function\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BitcoinTrainingAgent:\n",
    "    \"\"\"Q-learning agent.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        initial_epsilon: float,\n",
    "        epsilon_decay: float,\n",
    "        final_epsilon: float,\n",
    "        learning_rate: float = 0.001,\n",
    "        discount_factor: float = 0.95,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize hyperparameters\"\"\"\n",
    "        self.q_values = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "        self.lr = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "\n",
    "        self.training_error = []\n",
    "\n",
    "    def get_action(self, observation):\n",
    "        \"\"\"Given an observation, choose an action\"\"\"\n",
    "        # with probability epsilon return a random action to explore the environment\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "\n",
    "        # with probability (1 - epsilon) act greedily (exploit)\n",
    "        else:\n",
    "            return int(np.argmax(self.q_values[obs]))\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        obs: tuple[int, int, bool],\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        terminated: bool,\n",
    "        next_obs: tuple[int, int, bool],\n",
    "    ):\n",
    "        \"\"\"Updates the Q-value of an action.\"\"\"\n",
    "        future_q_value = (not terminated) * np.max(self.q_values[next_obs])\n",
    "        temporal_difference = (\n",
    "            reward + self.discount_factor * future_q_value - self.q_values[obs][action]\n",
    "        )\n",
    "\n",
    "        self.q_values[obs][action] = (\n",
    "            self.q_values[obs][action] + self.lr * temporal_difference\n",
    "        )\n",
    "        self.training_error.append(temporal_difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters passed to agent\n",
    "learning_rate = 0.01\n",
    "n_episodes = 1\n",
    "initial_epsilon = 1.0\n",
    "\n",
    "# Episolon is involved in exporation/exploitation tradeoff, decay reduces exploration over time.\n",
    "epsilon_decay = initial_epsilon / (n_episodes / 2)\n",
    "final_epsilon = 0.1\n",
    "\n",
    "agent = BitcoinTrainingAgent(\n",
    "    learning_rate=learning_rate,\n",
    "    initial_epsilon=initial_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    final_epsilon=final_epsilon,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model using agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : 423.10%   |   Portfolio Return : -94.73%   |   \n"
     ]
    }
   ],
   "source": [
    "# Run an episode until it ends :\n",
    "done, truncated = False, False\n",
    "observation, info = env.reset()\n",
    "while not done and not truncated:\n",
    "    # Pick a position by its index in your position list (=[-1, 0, 1])....usually something like : position_index = your_policy(observation)\n",
    "    position_index = env.action_space.sample() # At every timestep, pick a random position index from your position list (=[-1, 0, 1])\n",
    "    observation, reward, done, truncated, info = env.step(position_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Feature data_lr does not exist ... Check the available features : ['idx', 'step', 'date', 'position_index', 'position', 'real_position', 'data_close', 'data_high', 'data_low', 'data_date', 'data_open', 'data_vol', 'portfolio_valuation', 'portfolio_distribution_asset', 'portfolio_distribution_fiat', 'portfolio_distribution_borrowed_asset', 'portfolio_distribution_borrowed_fiat', 'portfolio_distribution_interest_asset', 'portfolio_distribution_interest_fiat', 'reward']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\gym_trading_env\\utils\\history.py:73\u001b[0m, in \u001b[0;36mHistory.__setitem__\u001b[1;34m(self, arg, value)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 73\u001b[0m     column_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mindex(column)\n\u001b[0;32m     74\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mValueError\u001b[0m: 'data_lr' is not in list",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[0;32m      6\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mget_action(obs)\n\u001b[1;32m----> 7\u001b[0m     next_obs, reward, terminated, truncated, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m      9\u001b[0m     agent\u001b[39m.\u001b[39mupdate(obs, action, reward, terminated, next_obs)\n\u001b[0;32m     11\u001b[0m     done \u001b[39m=\u001b[39m terminated \u001b[39mor\u001b[39;00m truncated\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\gym_trading_env\\environments.py:269\u001b[0m, in \u001b[0;36mTradingEnv.step\u001b[1;34m(self, position_index)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistorical_info\u001b[39m.\u001b[39madd(\n\u001b[0;32m    257\u001b[0m     idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_idx,\n\u001b[0;32m    258\u001b[0m     step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_step,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    266\u001b[0m     reward \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    267\u001b[0m )\n\u001b[0;32m    268\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[1;32m--> 269\u001b[0m     reward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreward_function(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhistorical_info)\n\u001b[0;32m    270\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistorical_info[\u001b[39m\"\u001b[39m\u001b[39mreward\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m reward\n\u001b[0;32m    272\u001b[0m \u001b[39mif\u001b[39;00m done \u001b[39mor\u001b[39;00m truncated:\n",
      "Cell \u001b[1;32mIn[57], line 50\u001b[0m, in \u001b[0;36mreward_function\u001b[1;34m(history)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreward_function\u001b[39m(history: History) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mfloat\u001b[39m:\n\u001b[0;32m     49\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"reward function for gym-trading-env\"\"\"\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     update_reward_columns(history)\n\u001b[0;32m     51\u001b[0m     average_log_return \u001b[39m=\u001b[39m history[\u001b[39m'\u001b[39m\u001b[39mdata_alr\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m     52\u001b[0m     var_sum \u001b[39m=\u001b[39m history[\u001b[39m'\u001b[39m\u001b[39mdata_var_sum\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "Cell \u001b[1;32mIn[57], line 18\u001b[0m, in \u001b[0;36mupdate_reward_columns\u001b[1;34m(history)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mif\u001b[39;00m history[\u001b[39m'\u001b[39m\u001b[39mposition\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m     17\u001b[0m     this_lr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlog(history[\u001b[39m'\u001b[39m\u001b[39mdata_close\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]) \u001b[39m-\u001b[39m np\u001b[39m.\u001b[39mlog(history[\u001b[39m'\u001b[39m\u001b[39mdata_close\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m])\n\u001b[1;32m---> 18\u001b[0m history\u001b[39m.\u001b[39;49m\u001b[39m__setitem__\u001b[39;49m((\u001b[39m'\u001b[39;49m\u001b[39mdata_lr\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m), this_lr) \u001b[39m# update history with new lr\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39m# running average of logarithmic return\u001b[39;00m\n\u001b[0;32m     22\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(history)\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\gym_trading_env\\utils\\history.py:75\u001b[0m, in \u001b[0;36mHistory.__setitem__\u001b[1;34m(self, arg, value)\u001b[0m\n\u001b[0;32m     73\u001b[0m     column_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mindex(column)\n\u001b[0;32m     74\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m---> 75\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFeature \u001b[39m\u001b[39m{\u001b[39;00mcolumn\u001b[39m}\u001b[39;00m\u001b[39m does not exist ... Check the available features : \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     76\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory_storage[:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize][t, column_index] \u001b[39m=\u001b[39m value\n",
      "\u001b[1;31mValueError\u001b[0m: Feature data_lr does not exist ... Check the available features : ['idx', 'step', 'date', 'position_index', 'position', 'real_position', 'data_close', 'data_high', 'data_low', 'data_date', 'data_open', 'data_vol', 'portfolio_valuation', 'portfolio_distribution_asset', 'portfolio_distribution_fiat', 'portfolio_distribution_borrowed_asset', 'portfolio_distribution_borrowed_fiat', 'portfolio_distribution_interest_asset', 'portfolio_distribution_interest_fiat', 'reward']"
     ]
    }
   ],
   "source": [
    "for episode in tqdm(range(n_episodes)):\n",
    "    obs, info = env.reset()\n",
    "    done, truncated = False, False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.get_action(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        agent.update(obs, action, reward, terminated, next_obs)\n",
    "\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "    \n",
    "    agent.decay_epsilon()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
