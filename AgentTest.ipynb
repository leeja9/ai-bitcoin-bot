{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Till Zemann\n",
    "# License: MIT License\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from matplotlib.patches import Patch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gymnasium as gym\n",
    "import gym_trading_env\n",
    "from gym_trading_env.utils.history import History\n",
    "from gym_trading_env.downloader import download\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create features\n",
    "\n",
    "- These are the state vector inputs\n",
    "- this would be an import from `ta_indicators`\n",
    "- only data columns starting with \"`feature_`\" will appear in the observation space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ta_indicators import * # Import the indicators\n",
    "\n",
    "# df is a DataFrame with columns : \"open\", \"high\", \"low\", \"close\", \"Volume USD\"\n",
    "\n",
    "# this can be defined in TAindicators and imported directly to the agent\n",
    "# after we get through testing things out in notebooks\n",
    "def define_state_vector(df: pd.DataFrame):\n",
    "    \"\"\"DEPRECATED.  I changed ta_indicators to put 'feature_' in front of all final indicators.\"\"\"\n",
    "\n",
    "    # Create the feature : ( close[t] - close[t-1] )/ close[t-1]\n",
    "    df[\"feature_close\"] = df[\"close\"].pct_change()\n",
    "\n",
    "    # Create the feature : open[t] / close[t]\n",
    "    df[\"feature_open\"] = df[\"open\"]/df[\"close\"]\n",
    "\n",
    "    # Create the feature : high[t] / close[t]\n",
    "    df[\"feature_high\"] = df[\"high\"]/df[\"close\"]\n",
    "\n",
    "    # Create the feature : low[t] / close[t]\n",
    "    df[\"feature_low\"] = df[\"low\"]/df[\"close\"]\n",
    "\n",
    "    # Create the feature : volume[t] / max(*volume[t-7*24:t+1])\n",
    "    df[\"feature_volume\"] = df[\"Volume USD\"] / df[\"Volume USD\"].rolling(7*24).max()\n",
    "\n",
    "    df.dropna(inplace= True) # Clean again !\n",
    "    # Eatch step, the environment will return 5 inputs  : \"feature_close\", \"feature_open\", \"feature_high\", \"feature_low\", \"feature_volume\"\n",
    "    \n",
    "\n",
    "df = pd.read_csv(\"./data/indicators.csv\")\n",
    "df.dropna(inplace= True)\n",
    "\n",
    "#df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define custom rewards and dynamic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_reward_columns(df: pd.DataFrame):\n",
    "    \"\"\"add reward columns to dataframe for incremental updates\"\"\"\n",
    "    for col in ['lr', 'alr', 'var_sum']:\n",
    "        df[col] = 0\n",
    "\n",
    "def update_reward_columns(history: History) -> None:\n",
    "    \"\"\"Set this episode lr, alr, var_sum, sr, powc\"\"\"\n",
    "\n",
    "    # Using weighted incremental algorithmic approach for average\n",
    "    # https://math.stackexchange.com/questions/106700/incremental-averaging\n",
    "    # general formula is: mean = ((n - 1) * last_mean + this_value) / n))\n",
    "\n",
    "    # logarithmic return\n",
    "    this_lr = 0\n",
    "    # if position is 1 (100% BTC)\n",
    "    if history['position', -1] == 1:\n",
    "        this_lr = np.log(history['data_close', -1]) - np.log(history['data_close', -2])\n",
    "    history.__setitem__(('data_lr', -1), this_lr) # update history with new lr\n",
    "\n",
    "\n",
    "    # running average of logarithmic return\n",
    "    n = len(history)\n",
    "    last_alr = history['data_alr', -2]\n",
    "    this_alr = ((n - 1) * last_alr + this_lr) / n\n",
    "    history.__setitem__(('data_alr', -1), this_alr) # update history with new alr\n",
    "\n",
    "    # running variance sum of logarithmic return\n",
    "    # for each nth row, dividing this sum by n gives population variance\n",
    "    last_alr = history['data_alr', -2]\n",
    "    last_var_sum = history['data_var_sum', -2]\n",
    "    this_var_sum = last_var_sum + abs((this_lr - last_alr) * (this_lr - this_alr))\n",
    "    history.__setitem__(('data_var_sum', -1), this_var_sum)\n",
    "\n",
    "def get_random_weights(arr_len):\n",
    "    \"\"\"get numpy array of random weights\"\"\"\n",
    "    max_val = 100\n",
    "    weight_vector = np.zeros(arr_len)\n",
    "    for i in range(arr_len - 1):\n",
    "        n = np.random.randint(0, max_val)\n",
    "        max_val = max_val - n\n",
    "        weight_vector[i] = n\n",
    "    weight_vector /= 100\n",
    "    weight_vector[-1] = 1 - sum(weight_vector[:-1])\n",
    "    np.random.shuffle(weight_vector)\n",
    "    return weight_vector\n",
    "\n",
    "\n",
    "def reward_function(history: History) -> float:\n",
    "    \"\"\"reward function for gym-trading-env\"\"\"\n",
    "    update_reward_columns(history)\n",
    "    average_log_return = history['data_alr', -1]\n",
    "    var_sum = history['data_var_sum', -1]\n",
    "    variance = var_sum / len(history)\n",
    "    std_dev = np.sqrt(variance)\n",
    "    sharpe_ratio = average_log_return / 0.5\n",
    "    this_lr = history['data_lr', -1]\n",
    "    powc = 0\n",
    "    # if this eposide position is 0 (100% USD) and last position was 1 (100% BTC)\n",
    "    # this compute time can also be traded for memory by adding a tracking column if needed\n",
    "    if (history['position', -1] == 0 and history['position', -2] == 1):\n",
    "        idx = history[-2]['idx']\n",
    "        \n",
    "        # This is an infinite loop if idx == 0 and history['position', idx] != 0.\n",
    "        while idx >= 0:\n",
    "            if (history['position', idx] == 0):\n",
    "                last_lr = history['data_lr', idx + 1]\n",
    "                powc = this_lr - last_lr\n",
    "    reward_vector = np.array([average_log_return, sharpe_ratio, powc])\n",
    "    weight_vector = get_random_weights(len(reward_vector))\n",
    "    reward = reward_vector @ weight_vector # dot product of random weights and reward values\n",
    "    return reward\n",
    "\n",
    "def dynamic_features(history: History) -> float:\n",
    "    \"\"\"Calculates dynamic features.\"\"\"\n",
    "    #dyn_features = [last_position, real_position]\n",
    "    #return dyn_features\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.89 0.   0.   0.   0.02 0.09]\n",
      "[0.   0.   0.09 0.02 0.89 0.  ]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "max_val = 100\n",
    "w = np.zeros(6)\n",
    "for i in range(5):\n",
    "    n = np.random.randint(0, max_val)\n",
    "    max_val = max_val - n\n",
    "    w[i] = n\n",
    "w /= 100\n",
    "w[-1] = 1 - sum(w[:-1])\n",
    "print(w)\n",
    "np.random.shuffle(w)\n",
    "print(w)\n",
    "print(sum(w))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>close</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>vol</th>\n",
       "      <th>feature_ROC_2</th>\n",
       "      <th>feature_ROC_4</th>\n",
       "      <th>feature_ROC_6</th>\n",
       "      <th>feature_ROC_8</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_ATR_2</th>\n",
       "      <th>feature_ATR_4</th>\n",
       "      <th>feature_ATR_6</th>\n",
       "      <th>feature_ATR_8</th>\n",
       "      <th>feature_ATR_16</th>\n",
       "      <th>feature_ATR_32</th>\n",
       "      <th>feature_ATR_64</th>\n",
       "      <th>lr</th>\n",
       "      <th>alr</th>\n",
       "      <th>var_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>15100.0</td>\n",
       "      <td>14917.0</td>\n",
       "      <td>15267.8</td>\n",
       "      <td>14100.0</td>\n",
       "      <td>21.32K</td>\n",
       "      <td>3.275263</td>\n",
       "      <td>14.165925</td>\n",
       "      <td>4.100870</td>\n",
       "      <td>1.623446</td>\n",
       "      <td>...</td>\n",
       "      <td>1121.10</td>\n",
       "      <td>1376.725</td>\n",
       "      <td>1637.716667</td>\n",
       "      <td>1645.2625</td>\n",
       "      <td>2044.16875</td>\n",
       "      <td>2029.025000</td>\n",
       "      <td>1375.210938</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>16999.0</td>\n",
       "      <td>15095.0</td>\n",
       "      <td>17170.0</td>\n",
       "      <td>14750.0</td>\n",
       "      <td>23.27K</td>\n",
       "      <td>13.202417</td>\n",
       "      <td>25.693765</td>\n",
       "      <td>25.352670</td>\n",
       "      <td>12.420934</td>\n",
       "      <td>...</td>\n",
       "      <td>1793.90</td>\n",
       "      <td>1734.225</td>\n",
       "      <td>1618.033333</td>\n",
       "      <td>1662.4750</td>\n",
       "      <td>2085.41875</td>\n",
       "      <td>2080.025000</td>\n",
       "      <td>1403.415625</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>2018-01-06</td>\n",
       "      <td>17067.0</td>\n",
       "      <td>16960.3</td>\n",
       "      <td>17132.2</td>\n",
       "      <td>16167.9</td>\n",
       "      <td>18.56K</td>\n",
       "      <td>13.305409</td>\n",
       "      <td>20.152726</td>\n",
       "      <td>28.641627</td>\n",
       "      <td>18.179089</td>\n",
       "      <td>...</td>\n",
       "      <td>1692.15</td>\n",
       "      <td>1406.625</td>\n",
       "      <td>1481.866667</td>\n",
       "      <td>1651.3250</td>\n",
       "      <td>1952.83125</td>\n",
       "      <td>2097.659375</td>\n",
       "      <td>1412.132812</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>2018-01-07</td>\n",
       "      <td>16180.0</td>\n",
       "      <td>17090.9</td>\n",
       "      <td>17090.9</td>\n",
       "      <td>15658.5</td>\n",
       "      <td>12.49K</td>\n",
       "      <td>-4.155853</td>\n",
       "      <td>11.522696</td>\n",
       "      <td>26.919518</td>\n",
       "      <td>27.575779</td>\n",
       "      <td>...</td>\n",
       "      <td>1198.35</td>\n",
       "      <td>1496.125</td>\n",
       "      <td>1555.600000</td>\n",
       "      <td>1513.1125</td>\n",
       "      <td>1752.10625</td>\n",
       "      <td>2082.121875</td>\n",
       "      <td>1425.732812</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>14944.2</td>\n",
       "      <td>16200.0</td>\n",
       "      <td>16287.6</td>\n",
       "      <td>13180.0</td>\n",
       "      <td>26.61K</td>\n",
       "      <td>-12.727139</td>\n",
       "      <td>1.077295</td>\n",
       "      <td>12.332664</td>\n",
       "      <td>21.176851</td>\n",
       "      <td>...</td>\n",
       "      <td>2270.00</td>\n",
       "      <td>1981.075</td>\n",
       "      <td>1694.416667</td>\n",
       "      <td>1678.9000</td>\n",
       "      <td>1830.14375</td>\n",
       "      <td>2074.859375</td>\n",
       "      <td>1469.440625</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date    close     open     high      low     vol  feature_ROC_2  \\\n",
       "128  2018-01-04  15100.0  14917.0  15267.8  14100.0  21.32K       3.275263   \n",
       "129  2018-01-05  16999.0  15095.0  17170.0  14750.0  23.27K      13.202417   \n",
       "130  2018-01-06  17067.0  16960.3  17132.2  16167.9  18.56K      13.305409   \n",
       "131  2018-01-07  16180.0  17090.9  17090.9  15658.5  12.49K      -4.155853   \n",
       "132  2018-01-08  14944.2  16200.0  16287.6  13180.0  26.61K     -12.727139   \n",
       "\n",
       "     feature_ROC_4  feature_ROC_6  feature_ROC_8  ...  feature_ATR_2  \\\n",
       "128      14.165925       4.100870       1.623446  ...        1121.10   \n",
       "129      25.693765      25.352670      12.420934  ...        1793.90   \n",
       "130      20.152726      28.641627      18.179089  ...        1692.15   \n",
       "131      11.522696      26.919518      27.575779  ...        1198.35   \n",
       "132       1.077295      12.332664      21.176851  ...        2270.00   \n",
       "\n",
       "     feature_ATR_4  feature_ATR_6  feature_ATR_8  feature_ATR_16  \\\n",
       "128       1376.725    1637.716667      1645.2625      2044.16875   \n",
       "129       1734.225    1618.033333      1662.4750      2085.41875   \n",
       "130       1406.625    1481.866667      1651.3250      1952.83125   \n",
       "131       1496.125    1555.600000      1513.1125      1752.10625   \n",
       "132       1981.075    1694.416667      1678.9000      1830.14375   \n",
       "\n",
       "     feature_ATR_32  feature_ATR_64  lr  alr  var_sum  \n",
       "128     2029.025000     1375.210938   0    0        0  \n",
       "129     2080.025000     1403.415625   0    0        0  \n",
       "130     2097.659375     1412.132812   0    0        0  \n",
       "131     2082.121875     1425.732812   0    0        0  \n",
       "132     2074.859375     1469.440625   0    0        0  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = pd.read_csv(\"./data/indicators.csv\")\n",
    "\n",
    "\"\"\"trainingDF = df.truncate(\n",
    "    after = pd.Timestamp('2023-01-01'),\n",
    "    copy = True\n",
    ")\n",
    "testingDF = df.truncate(\n",
    "    before = pd.Timestamp('2023-01-01'),\n",
    "    copy = True\n",
    ")\"\"\"\n",
    "add_reward_columns(df)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndf = pd.read_pickle('./data/binance-BTCUSDT-1h.pkl')\\ntrainingDF = df.truncate(\\n    after = pd.Timestamp('2023-01-01'),\\n    copy = True\\n)\\n\\ntrainingDF.dropna(inplace=True)\\ntrainingDF.sort_index(inplace=True)\\ntrainingDF.to_csv('./data/binance_training_data.csv')\\n\""
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "df = pd.read_pickle('./data/binance-BTCUSDT-1h.pkl')\n",
    "trainingDF = df.truncate(\n",
    "    after = pd.Timestamp('2023-01-01'),\n",
    "    copy = True\n",
    ")\n",
    "\n",
    "trainingDF.dropna(inplace=True)\n",
    "trainingDF.sort_index(inplace=True)\n",
    "trainingDF.to_csv('./data/binance_training_data.csv')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"df = pd.read_pickle('./data/binance-BTCUSDT-1h.pkl')\\ntestingDF = df.truncate(\\n    before = pd.Timestamp('2023-01-01'),\\n    copy = True\\n)\\n\\ntestingDF.dropna(inplace=True)\\ntestingDF.sort_index(inplace=True)\\ntestingDF.to_csv('./data/binance_testing_data.csv')\\n\""
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"df = pd.read_pickle('./data/binance-BTCUSDT-1h.pkl')\n",
    "testingDF = df.truncate(\n",
    "    before = pd.Timestamp('2023-01-01'),\n",
    "    copy = True\n",
    ")\n",
    "\n",
    "testingDF.dropna(inplace=True)\n",
    "testingDF.sort_index(inplace=True)\n",
    "testingDF.to_csv('./data/binance_testing_data.csv')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'download(\\n    exchange_names = [\"bitfinex2\", \"huobi\"],\\n    symbols= [\"BTC/USDT\"],\\n    timeframe= \"1h\",\\n    dir = \"data\",\\n    since= datetime.datetime(year= 2017, month= 1, day=1)\\n)'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"download(\n",
    "    exchange_names = [\"bitfinex2\", \"huobi\"],\n",
    "    symbols= [\"BTC/USDT\"],\n",
    "    timeframe= \"1h\",\n",
    "    dir = \"data\",\n",
    "    since= datetime.datetime(year= 2017, month= 1, day=1)\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"huobiDF = pd.read_pickle('./data/huobi-BTCUSDT-1h.pkl')\\nhuobi_training = huobiDF.truncate(after='2023-01-01')\\nhuobi_training.to_csv('./data/huobi-BTCUSDT-1h-training.csv')\\nhuobi_test = huobiDF.truncate(before='2023-01-01')\\nhuobi_test.to_csv('./data/huobi-BTCUSDT-1h-test.csv')\""
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"huobiDF = pd.read_pickle('./data/huobi-BTCUSDT-1h.pkl')\n",
    "huobi_training = huobiDF.truncate(after='2023-01-01')\n",
    "huobi_training.to_csv('./data/huobi-BTCUSDT-1h-training.csv')\n",
    "huobi_test = huobiDF.truncate(before='2023-01-01')\n",
    "huobi_test.to_csv('./data/huobi-BTCUSDT-1h-test.csv')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"bitfinex2DF = pd.read_pickle('./data/bitfinex2-BTCUSDT-1h.pkl')\\nbitfinex2_training = bitfinex2DF.truncate(after='2023-01-01')\\nbitfinex2_test = bitfinex2DF.truncate(before='2023-01-01')\\nbitfinex2_training.to_csv('./data/bitfinex2-BTCUSDT-1h-training.csv')\\nbitfinex2_test.to_csv('./data/bitfinex2-BTCUSDT-1h-test.csv')\""
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"bitfinex2DF = pd.read_pickle('./data/bitfinex2-BTCUSDT-1h.pkl')\n",
    "bitfinex2_training = bitfinex2DF.truncate(after='2023-01-01')\n",
    "bitfinex2_test = bitfinex2DF.truncate(before='2023-01-01')\n",
    "bitfinex2_training.to_csv('./data/bitfinex2-BTCUSDT-1h-training.csv')\n",
    "bitfinex2_test.to_csv('./data/bitfinex2-BTCUSDT-1h-test.csv')\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"TradingEnv\",\n",
    "        name= \"BTCUSD\",\n",
    "        df = df, # Your dataset with your custom features\n",
    "        positions = [0, 1], # -1 (=SHORT), 0(=SELL ALL), +1 (=BUY ALL)\n",
    "        #trading_fees = 0.01/100, # 0.01% per stock buy / sell (Binance fees)\n",
    "        #borrow_interest_rate= 0.0003/100, # 0.0003% per timestep (one timestep = 1h here)\n",
    "        #dynamic_feature_functions = [dynamic_features]\n",
    "        reward_function = reward_function,\n",
    "        portfolio_initial_value = 10000,\n",
    "        #max_episode_duration = 1000,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BitcoinTrainingAgent:\n",
    "    \"\"\"Q-learning agent.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        initial_epsilon: float,\n",
    "        epsilon_decay: float,\n",
    "        final_epsilon: float,\n",
    "        learning_rate: float = 0.001,\n",
    "        discount_factor: float = 0.95,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize hyperparameters\"\"\"\n",
    "        self.q_values = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "        self.lr = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "\n",
    "        self.training_error = []\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        \"\"\"Given an observation, choose an action\"\"\"\n",
    "        # with probability epsilon return a random action to explore the environment\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "\n",
    "        # with probability (1 - epsilon) act greedily (exploit)\n",
    "        else:\n",
    "            return int(np.argmax(self.q_values[obs]))\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        obs: tuple[int, int, bool],\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        terminated: bool,\n",
    "        next_obs: tuple[int, int, bool],\n",
    "    ):\n",
    "        \"\"\"Updates the Q-value of an action.\"\"\"\n",
    "        next_obs_tuple = tuple(next_obs)\n",
    "        obs_tuple = tuple(obs)\n",
    "        future_q_value = (not terminated) * np.max(self.q_values[next_obs_tuple])\n",
    "        temporal_difference = (\n",
    "            reward + self.discount_factor * future_q_value - self.q_values[obs_tuple][action]\n",
    "        )\n",
    "\n",
    "        self.q_values[obs_tuple][action] = (\n",
    "            self.q_values[obs_tuple][action] + self.lr * temporal_difference\n",
    "        )\n",
    "        self.training_error.append(temporal_difference)\n",
    "   \n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon * self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters passed to agent\n",
    "learning_rate = 0.01\n",
    "n_episodes = 1\n",
    "initial_epsilon = 1.0\n",
    "\n",
    "# Episolon is involved in exporation/exploitation tradeoff, decay reduces exploration over time.\n",
    "epsilon_decay = initial_epsilon / (n_episodes / 2)\n",
    "final_epsilon = 0.1\n",
    "\n",
    "agent = BitcoinTrainingAgent(\n",
    "    learning_rate=learning_rate,\n",
    "    initial_epsilon=initial_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    final_epsilon=final_epsilon,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model using agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Run an episode until it ends :\\ndone, truncated = False, False\\nobservation, info = env.reset()\\nwhile not done and not truncated:\\n    # Pick a position by its index in your position list (=[-1, 0, 1])....usually something like : position_index = your_policy(observation)\\n    position_index = env.action_space.sample() # At every timestep, pick a random position index from your position list (=[-1, 0, 1])\\n    observation, reward, done, truncated, info = env.step(position_index)\\n'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Run an episode until it ends :\n",
    "done, truncated = False, False\n",
    "observation, info = env.reset()\n",
    "while not done and not truncated:\n",
    "    # Pick a position by its index in your position list (=[-1, 0, 1])....usually something like : position_index = your_policy(observation)\n",
    "    position_index = env.action_space.sample() # At every timestep, pick a random position index from your position list (=[-1, 0, 1])\n",
    "    observation, reward, done, truncated, info = env.step(position_index)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step, loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "1 0\n",
      "2 0\n",
      "3 0\n",
      "4 0\n",
      "0 1\n",
      "1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:25<41:55, 25.41s/it]\n",
      "  0%|          | 0/1 [00:25<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[104], line 15\u001b[0m\n\u001b[0;32m     11\u001b[0m action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mget_action(obs)\n\u001b[0;32m     13\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m1\u001b[39m, i)\n\u001b[1;32m---> 15\u001b[0m next_obs, reward, terminated, truncated, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     17\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m2\u001b[39m, i)\n\u001b[0;32m     19\u001b[0m agent\u001b[39m.\u001b[39mupdate(obs, action, reward, terminated, next_obs)\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\gym_trading_env\\environments.py:269\u001b[0m, in \u001b[0;36mTradingEnv.step\u001b[1;34m(self, position_index)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistorical_info\u001b[39m.\u001b[39madd(\n\u001b[0;32m    257\u001b[0m     idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_idx,\n\u001b[0;32m    258\u001b[0m     step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_step,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    266\u001b[0m     reward \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    267\u001b[0m )\n\u001b[0;32m    268\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[1;32m--> 269\u001b[0m     reward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreward_function(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhistorical_info)\n\u001b[0;32m    270\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistorical_info[\u001b[39m\"\u001b[39m\u001b[39mreward\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m reward\n\u001b[0;32m    272\u001b[0m \u001b[39mif\u001b[39;00m done \u001b[39mor\u001b[39;00m truncated:\n",
      "Cell \u001b[1;32mIn[91], line 63\u001b[0m, in \u001b[0;36mreward_function\u001b[1;34m(history)\u001b[0m\n\u001b[0;32m     61\u001b[0m idx \u001b[39m=\u001b[39m history[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m][\u001b[39m'\u001b[39m\u001b[39midx\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     62\u001b[0m \u001b[39mwhile\u001b[39;00m idx \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m     \u001b[39mif\u001b[39;00m (history[\u001b[39m'\u001b[39;49m\u001b[39mposition\u001b[39;49m\u001b[39m'\u001b[39;49m, idx] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[0;32m     64\u001b[0m         last_lr \u001b[39m=\u001b[39m history[\u001b[39m'\u001b[39m\u001b[39mdata_lr\u001b[39m\u001b[39m'\u001b[39m, idx \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m]\n\u001b[0;32m     65\u001b[0m         powc \u001b[39m=\u001b[39m this_lr \u001b[39m-\u001b[39m last_lr\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\gym_trading_env\\utils\\history.py:42\u001b[0m, in \u001b[0;36mHistory.__getitem__\u001b[1;34m(self, arg)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__len__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m     41\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize\n\u001b[1;32m---> 42\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, arg):\n\u001b[0;32m     43\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(arg, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m     44\u001b[0m         column, t \u001b[39m=\u001b[39m arg\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for episode in tqdm(range(n_episodes)):\n",
    "    obs, info = env.reset()\n",
    "    done, truncated = False, False\n",
    "\n",
    "    while not done:\n",
    "        print(\"step, loop\")\n",
    "\n",
    "        for i in tqdm(range(100)):\n",
    "            print(0, i)\n",
    "\n",
    "            action = agent.get_action(obs)\n",
    "\n",
    "            print(1, i)\n",
    "\n",
    "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            print(2, i)\n",
    "\n",
    "            agent.update(obs, action, reward, terminated, next_obs)\n",
    "\n",
    "            print(3, i)\n",
    " \n",
    "            done = terminated or truncated\n",
    "            obs = next_obs\n",
    "\n",
    "            print(4, i)\n",
    "        \n",
    "    \n",
    "    agent.decay_epsilon()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
