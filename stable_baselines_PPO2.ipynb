{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "\n",
    "from gym_trading_env.utils.history import History\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from matplotlib.patches import Patch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gymnasium as gym\n",
    "import gym_trading_env\n",
    "from gym_trading_env.utils.history import History\n",
    "from gym_trading_env.downloader import download\n",
    "import datetime\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_reward_columns(df: pd.DataFrame):\n",
    "    \"\"\"add reward columns to dataframe for incremental updates\"\"\"\n",
    "    for col in ['lr', 'alr', 'var_sum']:\n",
    "        df[col] = 0\n",
    "\n",
    "\n",
    "def update_reward_columns(history: History) -> None:\n",
    "    \"\"\"Set this episode lr, alr, var_sum, sr, powc\"\"\"\n",
    "\n",
    "    # Using weighted incremental algorithmic approach for average\n",
    "    # https://math.stackexchange.com/questions/106700/incremental-averaging\n",
    "    # general formula is: mean = ((n - 1) * last_mean + this_value) / n))\n",
    "\n",
    "    # logarithmic return\n",
    "    this_lr = 0\n",
    "    # if position is 1 (100% BTC)\n",
    "    if history['position', -1] == 1:\n",
    "        this_lr = np.log(history['data_close', -1]) - \\\n",
    "            np.log(history['data_close', -2])\n",
    "    history.__setitem__(('data_lr', -1), this_lr)  # update history with new lr\n",
    "\n",
    "    # running average of logarithmic return\n",
    "    n = len(history)\n",
    "    last_alr = history['data_alr', -2]\n",
    "    this_alr = ((n - 1) * last_alr + this_lr) / n\n",
    "    # update history with new alr\n",
    "    history.__setitem__(('data_alr', -1), this_alr)\n",
    "\n",
    "    # running variance sum of logarithmic return\n",
    "    # for each nth row, dividing this sum by n gives population variance\n",
    "    last_alr = history['data_alr', -2]\n",
    "    last_var_sum = history['data_var_sum', -2]\n",
    "    this_var_sum = last_var_sum + \\\n",
    "        abs((this_lr - last_alr) * (this_lr - this_alr))\n",
    "    history.__setitem__(('data_var_sum', -1), this_var_sum)\n",
    "\n",
    "\n",
    "def get_random_weights(arr_len):\n",
    "    \"\"\"get numpy array of random weights\"\"\"\n",
    "    max_val = 100\n",
    "    weight_vector = np.zeros(arr_len)\n",
    "    for i in range(arr_len - 1):\n",
    "        n = np.random.randint(0, max_val)\n",
    "        max_val = max_val - n\n",
    "        weight_vector[i] = n\n",
    "    weight_vector /= 100\n",
    "    weight_vector[-1] = 1 - sum(weight_vector[:-1])\n",
    "    np.random.shuffle(weight_vector)\n",
    "    return weight_vector\n",
    "\n",
    "\n",
    "def reward_function(history: History) -> float:\n",
    "    \"\"\"reward function for gym-trading-env\"\"\"\n",
    "    update_reward_columns(history)\n",
    "    average_log_return = history['data_alr', -1]\n",
    "    var_sum = history['data_var_sum', -1]\n",
    "    variance = var_sum / len(history)\n",
    "    std_dev = np.sqrt(variance)\n",
    "    sharpe_ratio = average_log_return / 0.5\n",
    "    this_lr = history['data_lr', -1]\n",
    "    powc = 0\n",
    "    # if this eposide position is 0 (100% USD) and last position was 1 (100% BTC)\n",
    "    # this compute time can also be traded for memory by adding a tracking column if needed\n",
    "    if (history['position', -1] == 0 and history['position', -2] == 1):\n",
    "        idx = history[-2]['idx']\n",
    "\n",
    "        # This is an infinite loop if idx == 0 and history['position', idx] != 0.\n",
    "        while idx >= 0:\n",
    "            if (history['position', idx] == 0):\n",
    "                last_lr = history['data_lr', idx + 1]\n",
    "                powc = this_lr - last_lr\n",
    "    reward_vector = np.array([average_log_return, sharpe_ratio, powc])\n",
    "    weight_vector = get_random_weights(len(reward_vector))\n",
    "    # dot product of random weights and reward values\n",
    "    reward = reward_vector @ weight_vector\n",
    "    return reward\n",
    "\n",
    "\n",
    "def dynamic_features(history: History) -> float:\n",
    "    \"\"\"Calculates dynamic features.\"\"\"\n",
    "    # dyn_features = [last_position, real_position]\n",
    "    # return dyn_features\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv(\"./data/indicators.csv\")\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "add_reward_columns(df)\n",
    "\n",
    "env = gym.make(\"TradingEnv\",\n",
    "                     name=\"BTCUSD\",\n",
    "                     df=df, \n",
    "                     positions=[0, 1],\n",
    "                     # trading_fees = 0.01/100, # 0.01% per stock buy / sell (Binance fees)\n",
    "                     # borrow_interest_rate= 0.0003/100, # 0.0003% per timestep (one timestep = 1h here)\n",
    "                     # dynamic_feature_functions = [dynamic_features]\n",
    "                     reward_function=reward_function,\n",
    "                     portfolio_initial_value=10000,\n",
    "                     max_episode_duration = 1000,\n",
    "                     )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 11\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[39m=\u001b[39m PPO(\u001b[39m\"\u001b[39m\u001b[39mMlpPolicy\u001b[39m\u001b[39m\"\u001b[39m, env, \n\u001b[0;32m      2\u001b[0m             verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[0;32m      3\u001b[0m             gamma\u001b[39m=\u001b[39m\u001b[39m0.95\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m             batch_size\u001b[39m=\u001b[39m\u001b[39m15\u001b[39m,\n\u001b[0;32m      9\u001b[0m             )\n\u001b[1;32m---> 11\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:308\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[0;32m    300\u001b[0m     \u001b[39mself\u001b[39m: SelfPPO,\n\u001b[0;32m    301\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    306\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    307\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 308\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[0;32m    309\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[0;32m    310\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m    311\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[0;32m    312\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[0;32m    313\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[0;32m    314\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m    315\u001b[0m     )\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:259\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 259\u001b[0m     continue_training \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, callback, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrollout_buffer, n_rollout_steps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_steps)\n\u001b[0;32m    261\u001b[0m     \u001b[39mif\u001b[39;00m continue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m    262\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:178\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space, spaces\u001b[39m.\u001b[39mBox):\n\u001b[0;32m    176\u001b[0m     clipped_actions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(actions, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mlow, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mhigh)\n\u001b[1;32m--> 178\u001b[0m new_obs, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(clipped_actions)\n\u001b[0;32m    180\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mnum_envs\n\u001b[0;32m    182\u001b[0m \u001b[39m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:197\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \n\u001b[0;32m    193\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 197\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     56\u001b[0m     \u001b[39m# Avoid circular imports\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[1;32m---> 58\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mstep(\n\u001b[0;32m     59\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions[env_idx]\n\u001b[0;32m     60\u001b[0m         )\n\u001b[0;32m     61\u001b[0m         \u001b[39m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[0;32m     62\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx] \u001b[39m=\u001b[39m terminated \u001b[39mor\u001b[39;00m truncated\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\stable_baselines3\\common\\monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneeds_reset:\n\u001b[0;32m     93\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTried to step environment that needs reset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mappend(\u001b[39mfloat\u001b[39m(reward))\n\u001b[0;32m     96\u001b[0m \u001b[39mif\u001b[39;00m terminated \u001b[39mor\u001b[39;00m truncated:\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\gym_trading_env\\environments.py:269\u001b[0m, in \u001b[0;36mTradingEnv.step\u001b[1;34m(self, position_index)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistorical_info\u001b[39m.\u001b[39madd(\n\u001b[0;32m    257\u001b[0m     idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_idx,\n\u001b[0;32m    258\u001b[0m     step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_step,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    266\u001b[0m     reward \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    267\u001b[0m )\n\u001b[0;32m    268\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[1;32m--> 269\u001b[0m     reward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreward_function(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhistorical_info)\n\u001b[0;32m    270\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistorical_info[\u001b[39m\"\u001b[39m\u001b[39mreward\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m reward\n\u001b[0;32m    272\u001b[0m \u001b[39mif\u001b[39;00m done \u001b[39mor\u001b[39;00m truncated:\n",
      "Cell \u001b[1;32mIn[2], line 69\u001b[0m, in \u001b[0;36mreward_function\u001b[1;34m(history)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[39m# This is an infinite loop if idx == 0 and history['position', idx] != 0.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[39mwhile\u001b[39;00m idx \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m---> 69\u001b[0m     \u001b[39mif\u001b[39;00m (history[\u001b[39m'\u001b[39;49m\u001b[39mposition\u001b[39;49m\u001b[39m'\u001b[39;49m, idx] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[0;32m     70\u001b[0m         last_lr \u001b[39m=\u001b[39m history[\u001b[39m'\u001b[39m\u001b[39mdata_lr\u001b[39m\u001b[39m'\u001b[39m, idx \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m]\n\u001b[0;32m     71\u001b[0m         powc \u001b[39m=\u001b[39m this_lr \u001b[39m-\u001b[39m last_lr\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\gym_trading_env\\utils\\history.py:42\u001b[0m, in \u001b[0;36mHistory.__getitem__\u001b[1;34m(self, arg)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__len__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m     41\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize\n\u001b[1;32m---> 42\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, arg):\n\u001b[0;32m     43\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(arg, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m     44\u001b[0m         column, t \u001b[39m=\u001b[39m arg\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = PPO(\"MlpPolicy\", env, \n",
    "            verbose=0,\n",
    "            gamma=0.95, \n",
    "            n_steps=15, \n",
    "            ent_coef=0.01, \n",
    "            learning_rate=0.001,\n",
    "            clip_range=0.1, \n",
    "            batch_size=15,\n",
    "            )\n",
    "\n",
    "model.learn(total_timesteps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    env.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
