# TODO: Define reward behavior based on environment state
class BitcoinTrainingRewardAgent:
    def __init__(self) -> None:
        pass


# Reference:
#   Multi-Objective reward generalization: Improving performance of
#       Deep Reinforcement Learning for applications in single-asset trading
#   By: Federico Cornalba, Constantin Disselkamp,
#       Davide Scassola, Christopher Helf
#   URL: https://arxiv.org/abs/2203.04579
#   Companion repo: https://github.com/trality/fire

# ALGORITHM 1 PSEUDOCODE

# Algorithm 1 (DQN-HER)
# Input: MultiReward âˆˆ {True, False}
# Parameters: tol âˆˆ (0, 1), batchsize, ğ‘˜,ğ‘€ âˆˆ N, ğ‘† âŠ‚ {1, . . . ,ğ‘€}
# Output: Trained Multi-Reward agent.
# 1: Take one-hot encoding vector ğ’˜.
# 2: Initialize network ğ‘„ : (ğ’”,ğ’˜) â†¦â†’ [ğ‘„1(ğ’”,ğ’˜), . . . ,ğ‘„ğ‘ƒ (ğ’”,ğ’˜)] mapping
# state variables ğ’” and weight vector ğ’˜ to expected discounted
# return of every action ğ‘ âˆˆ {1, . . . , ğ‘ƒ}.
# 3: for ğ‘– = 1, . . . ,ğ‘€ do
# 4: Reset training environment.
# 5: while Episode ğ‘– not finished do
# 6: if MultiReward is True then
# 7: Sample random reward weights ğ’˜.
# 8: end if
# 9: if Unif(0, 1) < ğ‘¡ğ‘œğ‘™ then
# 10: Choose random action ğ‘
# 11: else
# 12: Pick ğ‘ â† arg maxğ‘ ğ‘„(ğ’”,ğ’˜).
# 13: end if
# 14: Conduct one step (get to new state ğ’”ğ‘›ğ‘’ğ‘¤)
# 15: Get associated reward ğ’˜ Â· ğ’“ (ğ’”, ğ‘)
# 16: Append single experience (ğ’”,ğ’˜,ğ’˜ Â· ğ’“ (ğ’”, ğ‘), ğ‘, ğ’”ğ‘›ğ‘’ğ‘¤) to experience
# replay R
# 17: Set ğ’” â† ğ’”ğ‘›ğ‘’ğ‘¤
# 18: if MultiReward is True then
# 19: Add other ğ‘˜ experiences to Replay by re-running lines
# 7-14-15-16 ğ‘˜ times.
# 20: end if
# 21: if ğ‘– âˆˆ ğ‘† then
# 22: Randomly sample batchsize units from R.
# 23: Update ğ‘„-values associated with each sampled unit
# using Bellman equation (2).
# 24: Fit model on estimated ğ‘„-values
# 25: end if
# 26: end while
# 27: end for

# ALGORITHM 2 PSEUDOCODE

# Algorithm 2 (DQN-HER) with discount factor generalization and
# random access point (Changes to lines of Algorithm 1)
# Input: Anchoring for cross-validation âˆˆ {True; False}
# 2: Initialize network ğ‘„ : (ğ’”,ğ’˜,ğ›¾) â†¦â†’ [ğ‘„1(ğ’”,ğ’˜,ğ›¾), . . . ,ğ‘„ğ‘ƒ (ğ’”,ğ’˜,ğ›¾)]
# mapping state variables ğ’”, weight vector ğ’˜, and discount factor
# ğ›¾ to expected discounted return of every action ğ‘ âˆˆ {1, . . . , ğ‘ƒ}.
# ...
# 4: if not Anchoring for cross-validation then
# 5: Randomly select subset of training set, and reset associated
# environment.
# 6: end if
# ...
# 7: Sample random reward weights ğ’˜ and discount factor ğ›¾.
# ...
# 12: Pick ğ‘ â† arg maxğ‘ ğ‘„(ğ’”,ğ’˜,ğ›¾).
# ...
# 16: Append single experience (ğ’”,ğ›¾,ğ’˜,ğ’˜ Â· ğ’“ (ğ’”, ğ‘), ğ‘, ğ’”ğ‘›ğ‘’ğ‘¤) to experience
# replay R
# ...
# 22: Randomly sample batchsize units from R, and normalize sampled ğ‘„-values
# according to Subsection 4.2.1.
