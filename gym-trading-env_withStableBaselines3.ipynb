{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import stable_baselines3\n",
    "\n",
    "import gymnasium as gym\n",
    "import gym_trading_env\n",
    "from gym_trading_env.utils.history import History\n",
    "from gym_trading_env.downloader import download\n",
    "import datetime\n",
    "\n",
    "from stable_baselines3.common.env_checker import check_env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = []\n",
    "for col in df.columns:\n",
    "    feature_list.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1863, 45)\n"
     ]
    }
   ],
   "source": [
    "def feature_cols(df):\n",
    "    df[\"feature_close\"] = df[\"close\"]\n",
    "    df[\"feature_high\"] = df[\"high\"]\n",
    "    df[\"feature_low\"] = df[\"low\"]\n",
    "    df[\"feature_open\"] = df[\"open\"]\n",
    "    df[\"feature_vol\"] = df[\"vol\"]\n",
    "\n",
    "def calculate_reward_cols(df, window_size):\n",
    "    prices = df[\"feature_close\"]\n",
    "    # This needs to calculate only when the model has a position.\n",
    "    log_returns = prices.pct_change().apply(lambda x: np.log(1 + x))\n",
    "    df[\"log_return\"] = log_returns\n",
    "    df[\"avg_log_return\"] = df[\"log_return\"].rolling(window=window_size).mean()\n",
    "    df[\"avg_log_return_std\"] = df[\"avg_log_return\"].std()\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"./data/indicators.csv\", parse_dates=[\"date\"], index_col=\"date\")\n",
    "feature_cols(df)\n",
    "#calculate_reward_cols(df, 30)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 3.2752628e+00,  1.4165925e+01,  4.1008701e+00,  1.6234456e+00,\n",
       "        -2.6530146e+01,  4.3732853e+01,  1.7506500e+02, -5.6983795e+01,\n",
       "        -2.0178050e+02, -7.5750748e+01, -1.2202590e+02, -1.4951251e+02,\n",
       "         4.6771442e+01,  6.7115637e+02,  6.6474899e+01,  1.4187622e+01,\n",
       "        -1.3082519e+01, -2.9601984e+01, -2.9691364e+01,  2.0186232e+01,\n",
       "        -4.2739563e+00,  5.0000000e+01,  4.5869595e+01,  3.6106823e+01,\n",
       "         3.2149414e+01,  2.9272116e+01,  3.6292324e+01,  3.7901905e+01,\n",
       "         1.1211000e+03,  1.3767250e+03,  1.6377167e+03,  1.6452625e+03,\n",
       "         2.0441687e+03,  2.0290250e+03,  1.3752109e+03,  1.5100000e+04,\n",
       "         1.5267800e+04,  1.4100000e+04,  1.4917000e+04,  2.1320000e+04,\n",
       "         0.0000000e+00,  0.0000000e+00], dtype=float32),\n",
       " {'idx': 0,\n",
       "  'step': 0,\n",
       "  'date': numpy.datetime64('2018-01-04T00:00:00.000000000'),\n",
       "  'position_index': 0,\n",
       "  'position': 0,\n",
       "  'real_position': 0,\n",
       "  'data_low': 14100.0,\n",
       "  'data_high': 15267.8,\n",
       "  'data_open': 14917.0,\n",
       "  'data_vol': 21320.0,\n",
       "  'data_close': 15100.0,\n",
       "  'portfolio_valuation': 1000.0,\n",
       "  'portfolio_distribution_asset': 0,\n",
       "  'portfolio_distribution_fiat': 1000.0,\n",
       "  'portfolio_distribution_borrowed_asset': 0,\n",
       "  'portfolio_distribution_borrowed_fiat': 0,\n",
       "  'portfolio_distribution_interest_asset': 0,\n",
       "  'portfolio_distribution_interest_fiat': 0,\n",
       "  'reward': 0})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\n",
    "    \"TradingEnv\",\n",
    "    name=\"BTCUSD\",\n",
    "    df=df,\n",
    "    positions=[0, 1],\n",
    "    )\n",
    "\n",
    "obs = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running env with random action taken each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : 44.32%   |   Portfolio Return : -40.79%   |   \n"
     ]
    }
   ],
   "source": [
    "done, truncated = False, False\n",
    "observation, info = env.reset()\n",
    "while not done and not truncated:\n",
    "    # Pick a position by its index in your position list (=[-1, 0, 1])....usually something like : position_index = your_policy(observation)\n",
    "    position_index = env.action_space.sample() # At every timestep, pick a random position index from your position list (=[-1, 0, 1])\n",
    "    observation, reward, done, truncated, info = env.step(position_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "----------------------------\n",
      "| time/              |     |\n",
      "|    fps             | 168 |\n",
      "|    iterations      | 1   |\n",
      "|    time_elapsed    | 0   |\n",
      "|    total_timesteps | 15  |\n",
      "----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 61           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 0            |\n",
      "|    total_timesteps      | 30           |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043043974 |\n",
      "|    clip_fraction        | 0.22         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -0.69        |\n",
      "|    explained_variance   | -17.2        |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | -0.0367      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0211      |\n",
      "|    value_loss           | 0.0535       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 68          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 0           |\n",
      "|    total_timesteps      | 45          |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001971662 |\n",
      "|    clip_fraction        | 0.0133      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -0.691      |\n",
      "|    explained_variance   | -4.84       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | -0.0231     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00972    |\n",
      "|    value_loss           | 0.0205      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 0           |\n",
      "|    total_timesteps      | 60          |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005919075 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -0.691      |\n",
      "|    explained_variance   | -0.14       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | -0.0266     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0163     |\n",
      "|    value_loss           | 0.008       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 75           |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 0            |\n",
      "|    total_timesteps      | 75           |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0147495475 |\n",
      "|    clip_fraction        | 0.467        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -0.682       |\n",
      "|    explained_variance   | 0.406        |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | -0.027       |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0165      |\n",
      "|    value_loss           | 0.00701      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 77           |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 1            |\n",
      "|    total_timesteps      | 90           |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023548563 |\n",
      "|    clip_fraction        | 0.00667      |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -0.677       |\n",
      "|    explained_variance   | -0.707       |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | -0.0191      |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00458     |\n",
      "|    value_loss           | 0.00888      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 77           |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 1            |\n",
      "|    total_timesteps      | 105          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027469378 |\n",
      "|    clip_fraction        | 0.0133       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -0.676       |\n",
      "|    explained_variance   | 0.35         |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | -0.0138      |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00547     |\n",
      "|    value_loss           | 0.0119       |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sb3_contrib.ppo_recurrent.ppo_recurrent.RecurrentPPO at 0x188059278d0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RecurrentPPO('MlpLstmPolicy', # feed-forward neural network with multiple hidden layers\n",
    "            env, # environment in which the agent interacts and learns\n",
    "            verbose=1, # enables the training progress to be printed during the learning process\n",
    "            gamma=0.95, # determines the importance of future rewards compared to immediate rewards\n",
    "            n_steps=15, # steps to collect samples from the environment before performing an update\n",
    "            ent_coef=0.01, # encourages exploration by adding entropy to the policy loss\n",
    "            learning_rate=0.001, # controls the step size at which model's parameters are updated based on the gradient of the loss function\n",
    "            clip_range=0.1, # limits the update to a certain range to prevent large policy updates\n",
    "            batch_size=15)\n",
    "\n",
    "model.learn(total_timesteps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m obs \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m):\n\u001b[1;32m----> 4\u001b[0m     action, _state \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(obs)\n\u001b[0;32m      5\u001b[0m     obs, reward, done, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\stable_baselines3\\common\\base_class.py:555\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\n\u001b[0;32m    536\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    537\u001b[0m     observation: Union[np\u001b[39m.\u001b[39mndarray, Dict[\u001b[39mstr\u001b[39m, np\u001b[39m.\u001b[39mndarray]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    540\u001b[0m     deterministic: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    541\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[np\u001b[39m.\u001b[39mndarray, Optional[Tuple[np\u001b[39m.\u001b[39mndarray, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]]]:\n\u001b[0;32m    542\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    543\u001b[0m \u001b[39m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[0;32m    544\u001b[0m \u001b[39m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[39m        (used in recurrent policies)\u001b[39;00m\n\u001b[0;32m    554\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 555\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy\u001b[39m.\u001b[39;49mpredict(observation, state, episode_start, deterministic)\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\sb3_contrib\\common\\recurrent\\policies.py:389\u001b[0m, in \u001b[0;36mRecurrentActorCriticPolicy.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[39m# Switch to eval mode (this affects batch norm / dropout)\u001b[39;00m\n\u001b[0;32m    387\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_training_mode(\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m--> 389\u001b[0m observation, vectorized_env \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobs_to_tensor(observation)\n\u001b[0;32m    391\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(observation, \u001b[39mdict\u001b[39m):\n\u001b[0;32m    392\u001b[0m     n_envs \u001b[39m=\u001b[39m observation[\u001b[39mlist\u001b[39m(observation\u001b[39m.\u001b[39mkeys())[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:260\u001b[0m, in \u001b[0;36mBaseModel.obs_to_tensor\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m    257\u001b[0m     observation \u001b[39m=\u001b[39m maybe_transpose(observation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space)\n\u001b[0;32m    259\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 260\u001b[0m     observation \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49marray(observation)\n\u001b[0;32m    262\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(observation, \u001b[39mdict\u001b[39m):\n\u001b[0;32m    263\u001b[0m     \u001b[39m# Dict obs need to be handled separately\u001b[39;00m\n\u001b[0;32m    264\u001b[0m     vectorized_env \u001b[39m=\u001b[39m is_vectorized_observation(observation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space)\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "\n",
    "for i in range(100):\n",
    "    action, _state = model.predict(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
